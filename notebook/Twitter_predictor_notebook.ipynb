{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing Phase"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing required modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# packages for importing and managing tweets\n",
    "import tweepy\n",
    "import snscrape.modules.twitter as sntwitter\n",
    "\n",
    "# basics of numerics, data structures, statistics\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from time import time\n",
    "import os\n",
    "import sys\n",
    "from pprint import pprint\n",
    "\n",
    "# importing other packages for text preprocessing\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "import nltk\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "import regex\n",
    "import string\n",
    "import random\n",
    "from gensim.models import Word2Vec\n",
    "import spacy\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "#from textblob import TextBlob\n",
    "import unicodedata\n",
    "import emoji  ## pay attention to the version... With version 0.6.0 works\n",
    "              ## with version 1.7 the attribute UNICODE_EMOJI does not exists -> EMOJI_DATA.\n",
    "\n",
    "\n",
    "# plotting and graphics\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# The Big package for Machine Learning\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.decomposition import NMF, LatentDirichletAllocation\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.decomposition import PCA\n",
    "# import the vectorizers from skearn:\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "# import a learning technique for the next step:\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.naive_bayes import MultinomialNB   #classifier\n",
    "# Import also a support vector machine (SVM)\n",
    "from sklearn import svm\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn import linear_model\n",
    "from sklearn.dummy import DummyRegressor\n",
    "# Importing functions for feature processing and analysis\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn import metrics\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import MaxAbsScaler  # for the sparse objects\n",
    "# Importing functions for accessing the models (from effectiveness axis)\n",
    "from sklearn.model_selection import *\n",
    "from sklearn.model_selection import validation_curve\n",
    "from sklearn.model_selection import RandomizedSearchCV, GridSearchCV"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Class for the authentication process for the Twitter API (v1.1 and v2.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing the script with the credentials for twitter as a module\n",
    "import twetter_credentials as credentials\n",
    "\n",
    "# create a class for the authentication for both the API versions, \n",
    "# taking the credentials from the imported script. Initializing the arguments as default \n",
    "# positional arguments, we can initialize the class without passing any argument in the object instantiation.\n",
    "# We can also initialize only one API version access (API or client, which requires only the bearer token).\n",
    "class authentication():\n",
    "    def __init__(self,  consumer_key=credentials.consumer_key, \n",
    "                        consumer_secret=credentials.consumer_secret, \n",
    "                        access_token= credentials.access_token, \n",
    "                        access_secret=credentials.access_secret, \n",
    "                        bearer_token= credentials.bearer_token        ):\n",
    "        self.consumer_key = consumer_key\n",
    "        self.consumer_secret = consumer_secret\n",
    "        self.access_token = access_token\n",
    "        self.access_secret = access_secret\n",
    "        self.bearer_token = bearer_token\n",
    "\n",
    "    # API v2.0 authentication via client\n",
    "    def client(self):\n",
    "        client = tweepy.Client( bearer_token = self.bearer_token,\n",
    "                                consumer_key = self.consumer_key,\n",
    "                                consumer_secret = self.consumer_secret,\n",
    "                                access_token = self.access_token,\n",
    "                                access_token_secret = self.access_secret,\n",
    "                                wait_on_rate_limit=True)\n",
    "        return client\n",
    "    \n",
    "    # API v1.1 authentication via API\n",
    "    def api(self):\n",
    "        auth = tweepy.OAuthHandler(self.consumer_key, self.consumer_secret)\n",
    "        auth.set_access_token(self.access_token, self.access_secret)\n",
    "        api = tweepy.API(auth, wait_on_rate_limit=True)\n",
    "        return api\n",
    "\n",
    "# The initialization is very fast:\n",
    "api = authentication().api()\n",
    "client = authentication().client()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a class for the search query object. Remember that there are two different query versions, depending on the API version used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dictionaries\n",
    "from dictionaries import context\n",
    "from dictionaries import food_dict\n",
    "\n",
    "# Create a class with some functions to build the searching query. The syntax for the searching query is different \n",
    "# among the two API versions (the reason for this is quite obfuscated), and also the keywords/parameters for advance \n",
    "# reaserch of tweets are also different. For example in the API v 1.1 there is no possibility to search a tweet with the \n",
    "# keyword `context` (that allows to search tweets belonging to a very large set of listed contexts and subcontexts) but only\n",
    "# giving the words related to a topic, with the proper logic operators. In the API v2.0 the query syntax offers the possibility \n",
    "# to search tweets directly specifing the `context` keyword (with the related number of the desired context to search). However,\n",
    "# there is no more the possibility to filter out the tweets using the number of retweets or replies.\n",
    "\n",
    "class query():\n",
    "\n",
    "    # Give a default value to the constructor's parameters. In this way, if we want only the query\n",
    "    # in v1.1 version, the class can be initialized passing only the dictionary and the number of tokens.\n",
    "    # For the query version v 2.0, we can pass only the context string.\n",
    "    def __init__(self, dictionary=None, number_of_tokens=None, context=None):\n",
    "        self.dictionary = dictionary\n",
    "        self.number_of_tokens = number_of_tokens\n",
    "        self.context = context\n",
    "\n",
    "\n",
    "    # Define a function to build a single query for the Cursor method in tweepy, in the form of a string.\n",
    "    # The query is built picking randomly a certain number_of_tokens from the dictionary, and writing them to \n",
    "    # the query. Passing this query to the Cursor(), the tweets with all the words selected are searched.\n",
    "    def make_query_api(self):\n",
    "        L = len(self.dictionary)\n",
    "        q = \"(\"\n",
    "        for i in range(self.number_of_tokens):\n",
    "            j = random.randint(0, L-1)\n",
    "            q = q + self.dictionary[j] + \" \"\n",
    "        # Append here all the other filters. For example we can specify the temporal interval, the minimum\n",
    "        # number of retweets or replies, and ask only for original tweets, excluding retweets and replies.\n",
    "        q = q + \") \" + \"min_retweets:3 min_replies:1 until:2022-12-31 since:2022-01-01 -filter:replies -filter:retweets lang:en\"\n",
    "        return q\n",
    "\n",
    "\n",
    "    # Define a function to make a query for the client Paginator() method in tweepy, in the form of a string.\n",
    "    # Since the context does not change, this query can be created once, calling this function, without the need\n",
    "    # of performin a `for` loop.\n",
    "    def make_query_client(self):\n",
    "        # With the API v2.0 we can search with the context keyword, but the possibility to filter with the \n",
    "        # minimum number of retweets or replies is unavailable. The possibility to specify the temporal interval\n",
    "        # is guaranteed by a parameter of the Paginator function.\n",
    "        q =  self.context + \" -is:reply -is:retweet lang:en\"\n",
    "        return q\n",
    "\n",
    "    # The context related number is listed in the twetter official web page. For example the food context number is 152. \n",
    "    # However, in the required format, it is mandatory to specify the sub-context/entity number. These (very large) numbers\n",
    "    # are not specified, and have to be discovered directly by the user, printing the `context_annotations` parameter of a tweet,\n",
    "    # returned by the `Paginator` method. This parameter, shows all the annotations about the entities and context of a certain tweet.\n",
    "    # A tweet can belong to different contexts and sub-contexts (or a mixture of them) -> all this information is reported.\n",
    "    def find_context(tweet):\n",
    "        print(tweet.context_annotations)\n",
    "        return\n",
    "\n",
    "\n",
    "    # Define a function to make a single query for the `StreamingClinet()` method in tweepy, in the form of a list.\n",
    "    # This client (v 2.0) method accepts queries only in the form of a list, and the parameter is even no more called query,\n",
    "    # in the tweepy' function definition.\n",
    "    def make_query_stream():\n",
    "        L = len(self.dictionary)\n",
    "        q = []\n",
    "        for i in range(self.number_of_tokens):\n",
    "            j = random.randint(0, L-1)\n",
    "            q.append(self.dictionary[j])\n",
    "        return q\n",
    "\n",
    "# The number 152 is the listed number for the food context -> context = 'context:152.825047692124442624' for food.\n",
    "\n",
    "# Initialize the query class object:\n",
    "query = query(dictionary=food_dict, number_of_tokens=2, context=context)\n",
    "# Call the functions and test if they work well\n",
    "query_api = query.make_query_api()\n",
    "query_client = query.make_query_client()\n",
    "#print(query_api, \"\\n\", query_client)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following class creates the dataset with all the tweets and metadata required. Moreover, it can expand the dataset adding all the features required for the ML system. Of course, it has a function for cleaning the text of the tweets. It uses, in a smart way, an initialized object of the previous query class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now create a class for the initial text_mining phase. The aim of the class is to provide functions to create the dataset\n",
    "# for all the implementations selected, and the API version selected. The first functions are able to create the entire dataset\n",
    "# with the lables of interest: the date of creation of the tweets, the authors, of course the text, the authors' followers, \n",
    "# and the fundamental metrics for predicting popularity: the number of retweets, the number of replies messages to the tweets, \n",
    "# and the number of quotes (that are the number of time a certain tweet has been mentioned in other tweets' text). \n",
    "# The number of likes metric has been excluded, because a tweet can be popular, but with a little number of likes, \n",
    "# maybe because the content is controversial or storng but non condivisible.\n",
    "\n",
    "class text_mining():\n",
    "\n",
    "    # Define the constructor operator, with default values set to None. In this way we are not \n",
    "    # obliged to parse arguments at the moment of initialization of the class object. In particular,\n",
    "    # if the class is intended to also create the dataset (and not only for cleaning it), only the query \n",
    "    # parameter has to be passed. The query parameter is an instantiation of the query class created above, \n",
    "    # not the query string created with that class. In fact, every function to created the dataset, will create \n",
    "    # the proper query string using the query-class functions with the instantiation provided in this constructor.\n",
    "    # If you want only to clean an already prepared dataset, pass only the `dataset` argument \n",
    "    # (better to be a Pandas DataFrame object type).\n",
    "    def __init__(self, query=None, dataset=None, corpus=None):\n",
    "        self.query = query\n",
    "        self.dataset = dataset\n",
    "        self.corpus = corpus\n",
    "    \n",
    "    # function to create a dataset for the tweets, using the API v2.0 via tweepy.client\n",
    "    # Non deprecated since it uses the API v2.0\n",
    "    def create_dataset_client(self):\n",
    "        \n",
    "        # Create the proper query string for API v2 and Paginator method, using the proper function of the query-class.\n",
    "        query_client = self.query.make_query_client()\n",
    "\n",
    "        # The Paginator() method is used for doing pagination, instead of tweepy.Cursor. This instruction creates pages of tweets. \n",
    "        # In particular `limit` pages, each page with `max_results` number of tweets. The method `flatten` simply transforms the \n",
    "        # page-structure in an array of tweets (with all the metadata specified) and returns a `limit` number of tweets.\n",
    "        # Maybe this is useful to avoid the rate limit error with Cursor and API v1.1. Notice that the tweets which can be \n",
    "        # imported with the API v1.1 are unlimited. Instead, the ones imported with the version 2.0 are limited to 2M.\n",
    "        # Moreover, with the `Paginator` method, we have to specify all the fields/attributes of tweets we want to import, \n",
    "        # with the `tweet_field` variable, unless they are not defined later (with Cursor is not required).\n",
    "        tweets = tweepy.Paginator(  client.search_recent_tweets, query=query_client, \n",
    "                                    tweet_fields=['created_at', 'text', 'id','author_id', 'public_metrics', 'entities', ], \n",
    "                                    #user_fields=['id', 'username','public_metrics'], \n",
    "                                    #expansions=['author_id', 'attachments.media_keys'], \n",
    "                                    max_results=100, limit=100                                ).flatten(limit=5000)\n",
    "\n",
    "        # Create a list with all the attributes, iterating on all the tweets returned by Paginator\n",
    "        tweets_list = [[    tweet.created_at,\n",
    "                            # with the API v2.0 seems cumbersome to get the users' followers number.\n",
    "                            # However, collected the tweets, the two API can be mixed up. We can use the \n",
    "                            # function get_user() from tAPI v1.1 that allows to get followers count in a simple way. \n",
    "                            # The identical function get_user() for the API v2.0 does not allow this (really obfuscated)\n",
    "                            api.get_user(user_id=tweet.author_id).name, \n",
    "                            tweet.text,\n",
    "                            api.get_user(user_id=tweet.author_id).followers_count,\n",
    "                            # the interesting attributes are contained in the public metrics varibale.\n",
    "                            # There practically no indication of this in the documentation (that is actually obfuscated).\n",
    "                            tweet.public_metrics['retweet_count'], \n",
    "                            tweet.public_metrics['reply_count'], \n",
    "                            tweet.public_metrics['quote_count']     ]   for tweet in tweets if (tweet.public_metrics['retweet_count']>=1)]\n",
    "\n",
    "\n",
    "        column_lables = [ 'date', 'author', 'text', 'author followers', 'retweets', 'replies', 'quotes' ]\n",
    "        dataset = pd.DataFrame(tweets_list, columns=column_lables)\n",
    "\n",
    "        # Select and store in a varibale the column of the dataset with the actual text of the tweets\n",
    "        tweets_text = dataset['text']\n",
    "        # Then upload the class attribute `dataset` (initially set to None) with the text column of the tweets,\n",
    "        # in order to use it in the next cleaning procedure.\n",
    "        self.dataset = dataset\n",
    "        self.corpus = tweets_text \n",
    "        return dataset\n",
    "\n",
    "    # The following function is able to create a dataset of tweets, without using the twetter API. To do so,\n",
    "    # the function uses the library `snscrape`, that limits the number of tweet to import to 1M. Moreover,\n",
    "    # the searching query uses the syntax of the API v1.1, with also the possibility to get the number of replies\n",
    "    # and the number of quotes, which is not possible with the tweepy Cursor.\n",
    "    def create_dataset_scraper(self):\n",
    "        \n",
    "        tweets_list = []\n",
    "        # define the number of iteration over the whole dictionary about food. Since it is not possible to search\n",
    "        # directly with the `context`, to get the tweets about food, we can build a different query every loop iteration\n",
    "        # choosing random words in the food's dictionary.\n",
    "        total_iterations = 580\n",
    "        items_per_query = 2\n",
    "\n",
    "        for i in range(total_iterations):\n",
    "            # Define the query in the API v1.1 standard (the actual one used in twetter app to search )\n",
    "            query_api = self.query.make_query_api()\n",
    "            # instantiation of the snscraper class\n",
    "            tweets = sntwitter.TwitterSearchScraper(query_api).get_items()\n",
    "            # attributes of interest: .date, .user.username, .user.followersCount, .content, .replyCount, .retweetCount, .quoteCount, media (count or even type)\n",
    "            for count,tweet in enumerate(tweets):\n",
    "                # define a variable for the media count:\n",
    "                if (tweet.media != None): media = len(tweet.media)\n",
    "                else: media = 0\n",
    "                tweets_list.append([    tweet.date, \n",
    "                                        tweet.user.username, \n",
    "                                        tweet.rawContent, \n",
    "                                        tweet.user.followersCount,\n",
    "                                        #tweet.user.friendsCount,\n",
    "                                        tweet.user.statusesCount,\n",
    "                                        tweet.user.favouritesCount,\n",
    "                                        #tweet.user.listedCount,\n",
    "                                        tweet.retweetCount,\n",
    "                                        tweet.replyCount,\n",
    "                                        tweet.quoteCount,\n",
    "                                        media                           ])\n",
    "                if (count == items_per_query): break\n",
    "        # create the dataset with Pandas as before:\n",
    "        column_lables = [   'date', 'author', 'text', \n",
    "                            'followers', \n",
    "                            #'friends', \n",
    "                            'status', \n",
    "                            'favourites',\n",
    "                            #'tot_listed', \n",
    "                            'retweets', 'replies', 'quotes', 'media' ]\n",
    "        dataset = pd.DataFrame(tweets_list, columns=column_lables)\n",
    "        # Select and store in a varibale the column of the dataset with the actual text of the tweets\n",
    "        # Then upload the class attribute `dataset` (initially set to None) with the text column of the tweets,\n",
    "        # in order to use it in the next cleaning procedure.\n",
    "        self.dataset = dataset\n",
    "        self.corpus = dataset['text']\n",
    "        return self.dataset\n",
    "\n",
    "\n",
    "    # define a function to create the dataset for the tweets using the API v1.1 with tweepy.\n",
    "    # Almost deprecated in favor of scraper.\n",
    "    def create_dataset_api(self):\n",
    "\n",
    "        tweets_list = []\n",
    "        total_iterations = 50\n",
    "        items_per_query = 4\n",
    "        # same structure as before\n",
    "        for i in range(total_iterations):\n",
    "            query_api = self.query.make_query_api()\n",
    "            tweets = tweepy.Cursor(api.search_tweets, q = query_api, lang = \"en\", tweet_mode='extended').items(items_per_query)\n",
    "\n",
    "            tweets_list = tweets_list + [   [tweet.created_at, \n",
    "                                            tweet.user.name,\n",
    "                                            tweet.user.followers_count,\n",
    "                                            tweet.text,\n",
    "                                            tweet.retweet_count,\n",
    "                                            tweet.favorite_count]   for tweet in tweets ]\n",
    "        # In this case the dataset is composed by the columns:\n",
    "        # date of pubblication, author, text, user's follower, retweets, likes\n",
    "        # With the API v1.1 for tweepy there is no possbility to get the number of replies and quotes\n",
    "        column_lables = [ 'date', 'author', 'author followers', 'text', 'retweets', 'likes' ]\n",
    "        dataset = pd.DataFrame(tweets_list, columns=column_lables)\n",
    "        # Select and store in a varibale the column of the dataset with the actual text of the tweets\n",
    "        tweets_text = dataset['text']\n",
    "        # Then upload the class attribute `dataset` (initially set to None) with the text column of the tweets,\n",
    "        # in order to use it in the next cleaning procedure.\n",
    "        self.dataset = dataset\n",
    "        self.corpus = tweets_text\n",
    "        return dataset\n",
    "\n",
    "\n",
    "    # The following function is quite similar (the structure is identical) as the one before.\n",
    "    # The only difference is that, in order to avoid (or try to) the problems with the limit rate\n",
    "    # of requests with the twetter API, the tweets imported are grouped in (two) pages instead of in single items.\n",
    "    # So the `Cursor` function is used as paginator with `Cursor().pages(number of pages)`, instead of with `.items()`.\n",
    "    # The number of tweets imported each cycle is not large, so 2 pages might be enough (or this function not so useful).\n",
    "    # Almost deprecated in favor of scraper...\n",
    "    def create_dataset_pages(self):\n",
    "\n",
    "        tweets_list = []    # we can construct a bigger dataset commenting this line, \n",
    "                            # and using more calls of the code box, that appends newer tweets in the list.\n",
    "        total_iterations = 50\n",
    "        items_per_page = 5\n",
    "        n_pages = 2\n",
    "\n",
    "        for i in range(total_iterations):\n",
    "            # To avoid problems with high number of reuqests, we can ask only for tweets that are actually retweets.\n",
    "            # The problem then is to find out the number of followers of the original author.\n",
    "            query_api = self.query.make_query_api()\n",
    "            # Use the same syntax as before to import a dataset of tweets about food:\n",
    "            tweets = tweepy.Cursor(api.search_tweets, q = query_api, lang = 'en', count=items_per_page, tweet_mode='extended').pages(n_pages)\n",
    "            # create a list for the tweets, including, respectively: date of creation, authot' username, actual text, number of retweets, number of likes.\n",
    "            for tweet in tweets:\n",
    "                for i in range(len(tweet)):\n",
    "                    tweets_list.append([tweet[i].created_at, \n",
    "                                        tweet[i].user.name,\n",
    "                                        tweet[i].user.followers_count,\n",
    "                                        tweet[i].text, \n",
    "                                        tweet[i].retweet_count, \n",
    "                                        tweet[i].favorite_count]    )\n",
    "\n",
    "        column_lables = [ 'date', 'author', 'author followers', 'text', 'retweets', 'likes' ]\n",
    "        dataset = pd.DataFrame(tweets_list, columns=column_lables)\n",
    "        # Select and store in a varibale the column of the dataset with the actual text of the tweets\n",
    "        tweets_text = dataset['text']\n",
    "        # Then upload the class attribute `dataset` (initially set to None) with the text column of the tweets,\n",
    "        # in order to use it in the next cleaning procedure.\n",
    "        self.dataset = dataset\n",
    "        self.corpus = tweets_text\n",
    "        return dataset\n",
    "\n",
    "    def expand(self):\n",
    "        #self.dataset['word_count'] = self.dataset['text'].apply(lambda x : len([word for word in str(x).split() if len(word)>1])) \n",
    "        self.dataset['word_count'] = self.dataset['text'].apply(lambda x : len(str(x).split()))\n",
    "        #self.dataset['char_count'] = self.dataset['text'].apply(lambda x : regex.findall(r'\\X', x)).apply(lambda i : len(i))\n",
    "        self.dataset['char_count'] = self.dataset['text'].apply(lambda x : len(x))\n",
    "        self.dataset['stop_words'] = self.dataset['text'].apply(lambda x : len([t for t in x.split() if t in STOP_WORDS]))\n",
    "        self.dataset['#tag'] = self.dataset['text'].apply(lambda x : len([t for t in x.split() if t.startswith('#')]))\n",
    "        self.dataset['@'] = self.dataset['text'].apply(lambda x : len([t for t in x.split() if t.startswith('@')]))\n",
    "        self.dataset['numaric'] = self.dataset['text'].apply(lambda x : len([t for t in x.split() if t.isdigit()]))\n",
    "        self.dataset['upercase'] = self.dataset['text'].apply(lambda x : len([t for t in x.split() if t.isupper()]))\n",
    "        # Extract the emails\n",
    "        #self.dataset['emails'] = self.dataset['text'].apply(lambda x : re.findall(r'([A-Za-z0-9+_-]+@[A-Za-z0-9+_-]+\\.[A-Za-z0-9+_-]+)', x))\n",
    "        # Count the emails\n",
    "        #self.dataset['emails_count'] = self.dataset['text'].apply(lambda x : re.findall(r'([A-Za-z0-9+_-]+@[A-Za-z0-9+_-]+\\.[A-Za-z0-9+_-]+)', x)).apply(lambda x : len(x))\n",
    "        # Count URL: \n",
    "        self.dataset['URLs'] = self.dataset['text'].apply(lambda i : re.findall(r'https?://[^\\s<>\"]+|www\\.[^\\s<>\"]+', i)).apply(lambda i : len(i))\n",
    "        # we can also count the number of emoji\n",
    "        self.dataset['emojis_count'] = self.dataset['text'].apply(lambda x : regex.findall(r'\\X', x)).apply(lambda x : len([char for char in x if(char in emoji.UNICODE_EMOJI)]))\n",
    "        # Count all punctiations and special characters from data, excluding the emojis already counted:\n",
    "        #self.dataset['special_char'] = self.dataset['text'].apply(lambda x : re.findall('[^A-Z a-z 0-9-]', x)).apply(lambda x : len(x))\n",
    "        # Extracting emotes. This might be useful for a tf-idf features apart from the ones with the pure text.\n",
    "        self.dataset['emojis'] = self.dataset['text'].apply(lambda x : regex.findall(r'\\X', x)).apply(lambda x : ''.join([char for char in x if(char in emoji.UNICODE_EMOJI)]))\n",
    "        return #self.dataset\n",
    "\n",
    "    # Define a function to clean the tweets. This function takes the single tweet and process it.\n",
    "    # Notice that this is not a class member function (does not contain the self attribute).\n",
    "    def tweet_cleaner(tweet):\n",
    "        # force to lower case\n",
    "        tweet = tweet.lower()\n",
    "        # substitute some unused/incompatible/strange chars with their used alias \n",
    "        # (for example for successfully applying the following expansions of the constracted forms)\n",
    "        for key in dictionaries.char_alias:\n",
    "            value = dictionaries.char_alias[key]\n",
    "            tweet = tweet.replace(key, value)\n",
    "        # expand the contracted forms, very frequent in english\n",
    "        for key in dictionaries.contractions:\n",
    "            value = dictionaries.contractions[key]\n",
    "            tweet = tweet.replace(key, value)\n",
    "        # again the lower case, because yes. It is needed...\n",
    "        tweet = tweet.lower()\n",
    "        # remove all the e-mails\n",
    "        tweet = re.sub(r'([A-Za-z0-9+_]+@[A-Za-z0-9+_]+\\.[A-Za-z0-9+_]+)',' ', tweet)\n",
    "        # remove the URLs\n",
    "        tweet = re.sub(r'https?://[^\\s<>\"]+|www\\.[^\\s<>\"]+',' ', tweet)\n",
    "        # remove mentions\n",
    "        tweet = re.sub('@[A-Za-z0-9_]+', '', tweet)\n",
    "        # removes all numbers\n",
    "        tweet = re.sub('[0-9]', '', tweet)\n",
    "        # remove other characters\n",
    "        #tweet = unicodedata.normalize('NFKD', tweet).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
    "        tweet = re.sub('[^A-Z a-z 0-9-]+','', tweet)\n",
    "        # remove extra spaces\n",
    "        tweet = \" \".join(tweet.split())\n",
    "        # remove the stopwords\n",
    "        #tweet = \" \".join(t for t in tweet.split() if t not in STOP_WORDS)\n",
    "        # stemming procedure\n",
    "        nlp = spacy.load('en_core_web_sm')\n",
    "        stem_sentence=[]\n",
    "        doc = nlp(tweet)\n",
    "        for token in doc:\n",
    "            stem_sentence.append(token.lemma_)\n",
    "            stem_sentence.append(\" \")\n",
    "        tweet = \"\".join(stem_sentence)\n",
    "        # lemmatization:\n",
    "        #nlp = spacy.load('en_core_web_sm')\n",
    "        #doc = nlp(tweet)\n",
    "        #lis = []\n",
    "        #for token in doc:\n",
    "        #    lemma = token.lemma_\n",
    "        #    if lemma == '-PRON-' or lemma == 'be':\n",
    "        #        lemma = token.text\n",
    "        #    lis.append(lemma)\n",
    "        #tweet = \" \".join(lis)\n",
    "        # We can correct spelling using textblob: \n",
    "        #tweet = str(TextBlob(tweet).correct())\n",
    "        return tweet\n",
    "\n",
    "    # The whole dataset function, to clean the whole dataset. This function is a class member function,\n",
    "    # and takes as parameter a cleaner function with the algorithm to process the single tweet.\n",
    "    def clean(self, cleaner_function=tweet_cleaner):\n",
    "        # re-define the class attribute `dataset` applying the function to clean the single tweet, and return it \n",
    "        self.corpus = self.corpus.apply(cleaner_function)\n",
    "        self.dataset['text'] = self.corpus\n",
    "        # Remove tweets that are too short ...\n",
    "        for i in range(len(self.dataset)):\n",
    "            if self.dataset['word_count'][i] < 5:\n",
    "                self.dataset = self.dataset.drop(i, axis=0)\n",
    "        self.dataset = self.dataset.reset_index(drop=True)\n",
    "        # Remove tweets that contains strange chars, such as chars from different alphabets.\n",
    "        #undecoded_chars = self.dataset['text'].apply(lambda x : re.findall('[^\\w\\s]', x)).apply(lambda i : len(i)) - self.dataset['emojis_count']\n",
    "        #for i in range(len(self.dataset)):\n",
    "        #    if self.dataset['special_char'][i] != undecoded_chars[i]:\n",
    "        #        self.dataset = self.dataset.drop(i, axis=0)\n",
    "        #self.dataset = self.dataset.reset_index()\n",
    "        #### A function that removes tweets that have been modified too much is very desired.... \n",
    "        #### The stopwords might be a problem in this case, but will'see.\n",
    "        return self.dataset\n",
    "\n",
    "\n",
    "# Initialize the text_mining class object, passing the instantiation of the query class\n",
    "miner = text_mining(query=query)\n",
    "\n",
    "dataset = miner.create_dataset_scraper()\n",
    "# save to csv\n",
    "dataset.to_csv(path_or_buf=\"./raw_dataset.csv\")\n",
    "#display(dataset)\n",
    "#display(miner.dataset)\n",
    "\n",
    "miner.expand()\n",
    "dataset_clean = miner.clean()\n",
    "dataset.to_csv(path_or_buf=\"./clean_dataset.csv\")\n",
    "display(dataset_clean)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we can import previous saved datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import an existing clean dataset:\n",
    "#dataset_clean = pd.read_csv('combined-files.csv')\n",
    "#dataset_clean = dataset_clean.drop('Unnamed: 19', axis = 1)\n",
    "#display(dataset_clean)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Further cleaning, and eventually selecting for the mediatic impact of the author."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# eventually remove the tweets with <2 retweets.\n",
    "for i in range(len(dataset_clean)):\n",
    "    if (dataset_clean['retweets'][i] < 2):\n",
    "        dataset_clean = dataset_clean.drop(i, axis = 0)\n",
    "dataset_clean = dataset_clean.reset_index(drop = True)\n",
    "\n",
    "\n",
    "# Also, interpreting the popularity of a tweet as the success (this last thing is more interesting from the point of view \n",
    "# of the practical usage for a corporate client), we can take only the tweets belonging to a certain mediatic impact (measured\n",
    "# with the number of followers or other indexes). In this case we can take a range of followers, and measure the regression metric \n",
    "# in base of these tweets. In this case having the benchmark with the dummy regressor (which returns the mean) can be very useful.\n",
    "mediatic_impact = 10000\n",
    "for i in range(len(dataset_clean)):\n",
    "    if (dataset_clean['followers'][i] > mediatic_impact + mediatic_impact*0.3 or dataset_clean['followers'][i] < mediatic_impact - mediatic_impact*0.3):\n",
    "        dataset_clean = dataset_clean.drop(i, axis = 0)\n",
    "dataset_clean = dataset_clean.reset_index(drop = True)\n",
    "#display(dataset_clean)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scatter matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Features analysis: create the scatter-plot matrix\n",
    "dataset_plot = dataset_clean.drop(['date', 'author', 'text', 'favourites', 'status', 'quotes', 'char_count', 'word_count', 'stop_words'], axis=1)\n",
    "scatter_matrix = pd.plotting.scatter_matrix(dataset_plot, figsize=(20,20))\n",
    "scatter_matrix\n",
    "plt.savefig(\"./scatter_matrix.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scatter matrix with the actual used features\n",
    "dataset_plot['target'] = np.log((dataset_plot['retweets'] + dataset_plot['replies']).to_numpy())\n",
    "dataset_plot = dataset_plot.drop(['retweets', 'replies', 'URLs'], axis = 1)\n",
    "dataset_plot['followers'] = np.log(dataset_plot['followers'].to_numpy())\n",
    "\n",
    "scatter_matrix = pd.plotting.scatter_matrix(dataset_plot, figsize=(16,16))\n",
    "scatter_matrix\n",
    "plt.savefig(\"./scatter_matrix_log.png\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Class made for cluster analysis with Unsupervised learning techniques. This is useful to analyze the quality of the imported tweets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class cluster():\n",
    "\n",
    "    def __init__(       self, \n",
    "                        number_of_clusters = 8, \n",
    "                        number_top_words = 6, \n",
    "                        batch_size = 64, \n",
    "                        init = \"nndsvda\", \n",
    "                        dataset = None, \n",
    "                        *args, **kwargs     ):\n",
    "\n",
    "        self.number_of_clusters = number_of_clusters\n",
    "        self.number_top_words = number_top_words\n",
    "        self.batch_size = batch_size\n",
    "        self.init = init\n",
    "        self.dataset = dataset\n",
    "    \n",
    "    def plot_top_words(model, feature_names, n_top_words, title):\n",
    "        fig, axes = plt.subplots(2, 4, figsize=(10, 5), sharex=True)\n",
    "        axes = axes.flatten()\n",
    "        for topic_idx, topic in enumerate(model.components_):\n",
    "            top_features_ind = topic.argsort()[: -n_top_words - 1 : -1]\n",
    "            top_features = [feature_names[i] for i in top_features_ind]\n",
    "            weights = topic[top_features_ind]\n",
    "\n",
    "            ax = axes[topic_idx]\n",
    "            ax.barh(top_features, weights, height=0.7)\n",
    "            ax.set_title(f\"Topic {topic_idx +1}\", fontdict={\"fontsize\": 8})\n",
    "            ax.invert_yaxis()\n",
    "            ax.tick_params(axis=\"both\", which=\"major\", labelsize=8)\n",
    "            for i in \"top right left\".split():\n",
    "                ax.spines[i].set_visible(False)\n",
    "            fig.suptitle(title, fontsize=10)\n",
    "\n",
    "        plt.subplots_adjust(top=0.90, bottom=0.05, wspace=0.90, hspace=0.3)\n",
    "        plt.show()\n",
    "\n",
    "    def NMF(self, plotting_function=plot_top_words):\n",
    "        # Use tf-idf features for NMF.\n",
    "        print(\"Extracting tf-idf features for NMF...\")\n",
    "        tfidf_vectorizer = TfidfVectorizer( max_df=0.95, min_df=2, stop_words=\"english\" )\n",
    "\n",
    "        t0 = time()\n",
    "        tfidf = tfidf_vectorizer.fit_transform(self.dataset)\n",
    "        print(\"done in %0.3fs.\" % (time() - t0))\n",
    "\n",
    "        # Fit the NMF model\n",
    "        print( \"Fitting the NMF model (Frobenius norm) with tf-idf features\" )\n",
    "\n",
    "        nmf = NMF(\n",
    "            n_components=self.number_of_clusters,\n",
    "            random_state=1,\n",
    "            init=self.init,\n",
    "            beta_loss=\"frobenius\",\n",
    "            alpha_W=0.00005,\n",
    "            alpha_H=0.00005,\n",
    "            l1_ratio=1,\n",
    "        ).fit(tfidf)\n",
    "\n",
    "        tfidf_feature_names = tfidf_vectorizer.get_feature_names_out()\n",
    "        plotting_function(nmf, tfidf_feature_names, self.number_top_words, \"Topics in NMF model (Frobenius norm)\" )\n",
    "\n",
    "        # Fit the NMF model\n",
    "        print(\"Fitting the NMF model (generalized Kullback-Leibler divergence) with tf-idf features\" )\n",
    "\n",
    "        nmf = NMF(\n",
    "            n_components=self.number_of_clusters,\n",
    "            random_state=1,\n",
    "            init=self.init,\n",
    "            beta_loss=\"kullback-leibler\",\n",
    "            solver=\"mu\",\n",
    "            max_iter=1000,\n",
    "            alpha_W=0.00005,\n",
    "            alpha_H=0.00005,\n",
    "            l1_ratio=0.5,\n",
    "        ).fit(tfidf)\n",
    "\n",
    "        tfidf_feature_names = tfidf_vectorizer.get_feature_names_out()\n",
    "        plotting_function( nmf, tfidf_feature_names, self.number_top_words, \"Topics in NMF model (generalized Kullback-Leibler divergence)\")\n",
    "\n",
    "    def LDA(self, plotting_function=plot_top_words):\n",
    "        # Use tf (raw term count) features for LDA.\n",
    "        print(\"Extracting tf features for LDA...\")\n",
    "        tf_vectorizer = CountVectorizer( max_df=0.95, min_df=2, stop_words=\"english\" )\n",
    "\n",
    "        t0 = time()\n",
    "        tf = tf_vectorizer.fit_transform(self.dataset)\n",
    "        print(\"done in %0.3fs.\" % (time() - t0))\n",
    "\n",
    "        print( \"Fitting LDA models with tf features\" )\n",
    "        lda = LatentDirichletAllocation(\n",
    "            n_components=self.number_of_clusters,\n",
    "            max_iter=5,\n",
    "            learning_method=\"online\",\n",
    "            learning_offset=50.0,\n",
    "            random_state=0,\n",
    "        )\n",
    "\n",
    "        t0 = time()\n",
    "        lda.fit(tf)\n",
    "        print(\"done in %0.3fs.\" % (time() - t0))\n",
    "\n",
    "        tf_feature_names = tf_vectorizer.get_feature_names_out()\n",
    "        plotting_function(lda, tf_feature_names, self.number_top_words, \"Topics in LDA model\")\n",
    "\n",
    "\n",
    "clusters = cluster(dataset=dataset_clean['text'])\n",
    "clusters.NMF()\n",
    "clusters.LDA()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning Phase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a function of the class to insert these stuff. Call it \"make_features_target\"\n",
    "target_data = dataset_clean[['retweets', 'replies', 'quotes']]\n",
    "features_data = dataset_clean.drop([\"date\", \"author\", \"retweets\", \"replies\", \"quotes\", \"status\", \"favourites\", \"char_count\", \"word_count\", \"stop_words\"], axis=1)\n",
    "# Taking the log also of the followers. As we can see from the scatterplot matrix, also the distribution of the followers count is skewed to the left. \n",
    "features_data['followers'] = np.log(features_data['followers'].to_numpy())\n",
    "print(target_data.shape, features_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate the vectorizer as an object of the class `TfidfVectorizer`, and assign it to the varibale vectorizer.\n",
    "vectorizer = TfidfVectorizer(max_df=0.95, min_df=1, stop_words=None, ngram_range=(2,5))\n",
    "\n",
    "# One may have the crazy idea to use tfidf also on the emojis. This can be particularly useful with tweets with a lot of emojis.\n",
    "#vectorizer_emojis = TfidfVectorizer(ngram_range=(1,5), stop_words = None, token_pattern=r'[^\\s]')\n",
    "\n",
    "# Now try to build some more sofisticated thing, using column transform. This is now a sort of obliged step, since we have several columns of different features. \n",
    "# The main problem is that the features related to the pure text, that are the tf-idf features, are created by the sklearn with an object of the type csx_sparse matrix of \n",
    "# scipy. This object is a unique block: cannot be divided in rows of a dataset, because looses the type of spoarse matrix (becomes a list of series objects) and the LT \n",
    "# of sklearn complains (wants pandas dataframe, sparse matrix or np arrays). So, the possible choices are 2: or trasform all the other columns of features into sparse objects and adding \n",
    "# to the tf-idf matrix, or using the column transformer and a pipeline. This second option allows also to normalize all the columns, that is a quite common procedure in text mining. \n",
    "normalize = Normalizer()\n",
    "standard_scaler = StandardScaler()\n",
    "hot_encoder = OneHotEncoder()\n",
    "# Build the trasformation:\n",
    "transformer = ColumnTransformer(\n",
    "    [\n",
    "        ('tfidf', vectorizer, 'text'),\n",
    "        #('tfidfEmotes', vectorizer_emojis, 'emojis'),\n",
    "        #('normalize', normalize, ['followers']),\n",
    "        #('categorical_encoding', hot_encoder, ['media', '#tag', '@', 'numaric', 'upercase', 'URLs'])\n",
    "    ], remainder = standard_scaler)\n",
    "\n",
    "# Create the features:\n",
    "features = transformer.fit_transform(features_data)\n",
    "#display(pd.DataFrame(features))\n",
    "\n",
    "# This is only to see the dictionary of words, used for tfidf features.\n",
    "vectorizer.fit_transform(features_data['text'])\n",
    "print(vectorizer.get_feature_names_out())\n",
    "# The same thing for the eventual tfidf for the emojis.\n",
    "#vectorizer_emojis.fit_transform(features_data['emojis'])\n",
    "#print(vectorizer_emojis.get_feature_names_out())\n",
    "\n",
    "# Create the target, and apply some transformations:\n",
    "target = target_data['retweets'] + target_data['replies'] #+ target_data['quotes']\n",
    "# Explore the distribution of the targets (non transformed)\n",
    "target.plot.hist(grid=True, bins=60, rwidth=0.8, color='#607c8e')\n",
    "# Since the distribution of the targets is very skewed to the left, take a log transformation, \n",
    "# in order to make the distribution more bell-shaped.\n",
    "target = np.log(target)\n",
    "target.plot.hist(grid=True, bins=60, rwidth=0.8, color='#607c8e')\n",
    "# The situation is in fact impreved really well. So, in the further part of the program, \n",
    "# the targets are considered to be transformed with the logarithm."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the selected models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_list = [\n",
    "    DummyRegressor(),\n",
    "    RandomForestRegressor(),   # better with the test bag\n",
    "    svm.SVR(kernel = 'poly', coef0 = 1.2, C = 1.0, epsilon = 0.1),\n",
    "    #linear_model.LinearRegression(),   #0.011\n",
    "    #linear_model.PoissonRegressor(),   #0.38\n",
    "    linear_model.Ridge(alpha=0.001), #0.045 , better with lower alpha\n",
    "    #linear_model.Lasso(alpha=0.001),   #0.41\n",
    "]\n",
    "\n",
    "train_comparing_targets = pd.DataFrame(target)\n",
    "errors_train = pd.DataFrame()\n",
    "\n",
    "for model in models_list:\n",
    "\n",
    "    model.fit(features, target)\n",
    "    predicts = model.predict(features)\n",
    "\n",
    "    model_str = '{}'.format(model)\n",
    "    train_comparing_targets[model_str] = predicts\n",
    "\n",
    "    #print(metrics.mean_absolute_percentage_error(target, predicts))\n",
    "    errors_train[model_str] = [ metrics.mean_absolute_percentage_error(target, predicts), \n",
    "                                metrics.r2_score(target, predicts),\n",
    "                                np.sqrt(metrics.mean_squared_error(target, predicts)),\n",
    "                                metrics.mean_squared_error(target, predicts),\n",
    "                                ]\n",
    "\n",
    "display(train_comparing_targets)\n",
    "display(errors_train)\n",
    "train_comparing_targets.to_csv(path_or_buf=\"./train_target_comparing.csv\")\n",
    "errors_train.to_csv(path_or_buf=\"./train_errors.csv\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now perform the training, and the assesing with a test set. In particular, in this case we can take a simple split of the dataset between train and test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# splitting the whole dataset into train set and test set. In this case using a simple static division\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, target, test_size=0.2, random_state=0)\n",
    "\n",
    "test_comparing_targets = pd.DataFrame(y_test)\n",
    "errors_test = pd.DataFrame()\n",
    "\n",
    "for model in models_list:\n",
    "\n",
    "    model.fit(X_train, y_train)\n",
    "    predicts = model.predict(X_test)\n",
    "\n",
    "    model_str = '{}'.format(model)\n",
    "    test_comparing_targets[model_str] = predicts\n",
    "\n",
    "    errors_test[model_str] =[   metrics.mean_absolute_percentage_error(y_test, predicts), \n",
    "                                metrics.r2_score(y_test, predicts),\n",
    "                                np.sqrt(metrics.mean_squared_error(y_test, predicts)),\n",
    "                                metrics.mean_squared_error(y_test, predicts)\n",
    "                            ]\n",
    "\n",
    "display(test_comparing_targets)\n",
    "display(errors_test)\n",
    "test_comparing_targets.to_csv(path_or_buf=\"./test_target_comparing.csv\")\n",
    "errors_test.to_csv(path_or_buf=\"./test_errors.csv\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now try to assess again the Learning techniques, but with the cross validation technique. We can take for example 5-fold CV or 10-fold CV, depending also on the computational time, so on the efficiency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_train = pd.DataFrame()\n",
    "cv_test = pd.DataFrame()\n",
    "\n",
    "metrics_list = ['neg_mean_absolute_percentage_error', 'r2', 'neg_root_mean_squared_error', 'neg_mean_squared_error']\n",
    "\n",
    "for model in models_list:\n",
    "    model_str = '{}'.format(model)\n",
    "    # train the models and perform cross-validation\n",
    "    cross_validation = cross_validate(model, features, target, scoring = metrics_list, cv = 5, return_train_score = True, n_jobs = -1)\n",
    "    # create the datasets with the metrics/scores selected.\n",
    "    cv_train[model_str] = [np.mean(cross_validation['train_'+ metric]) for metric in metrics_list]\n",
    "    cv_test[model_str] = [np.mean(cross_validation['test_'+ metric]) for metric in metrics_list]\n",
    "\n",
    "display(cv_train)\n",
    "display(cv_test)\n",
    "cv_train.to_csv(path_or_buf=\"./cv_train.csv\")\n",
    "cv_test.to_csv(path_or_buf=\"./cv_test.csv\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try to plot some interesting things. This can be useful to assess the learning techniques, and analyze the flexibility against the effectiveness. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model in models_list:\n",
    "    _, train_scores, test_scores, fit_times, score_times = learning_curve(  estimator = model, \n",
    "                                                                            X = features, \n",
    "                                                                            y = target, \n",
    "                                                                            cv = 5,\n",
    "                                                                            n_jobs = -1, \n",
    "                                                                            scoring = 'neg_mean_absolute_percentage_error', \n",
    "                                                                            return_times = True\n",
    "                                                                        )\n",
    "    # transform the error in accuracy. Remember that with this function the error calculated is -MAPE.\n",
    "    train_scores = 1 + train_scores\n",
    "    test_scores = 1 + test_scores\n",
    "\n",
    "    # create the figues:\n",
    "    fig, (ax1, ax2) = plt.subplots(nrows=1, ncols=2, figsize=(16, 6))\n",
    "    ax1.plot(fit_times.mean(axis=1), train_scores.mean(axis=1), \"o-\", label = 'train_score', color = 'green')\n",
    "    ax1.fill_between(\n",
    "        fit_times.mean(axis=1),\n",
    "        train_scores.mean(axis=1) - train_scores.std(axis=1),\n",
    "        train_scores.mean(axis=1) + train_scores.std(axis=1),\n",
    "        alpha=0.3,\n",
    "        color = 'green'\n",
    "    )\n",
    "    ax1.set_ylabel(\"Accuracy\")\n",
    "    ax1.set_xlabel(\"Fit time (s)\")\n",
    "    ax1.set_title(f\"train_score of {model.__class__.__name__} \")\n",
    "    ax2.plot(fit_times.mean(axis=1), test_scores.mean(axis=1), \"o-\", label = 'cross_validation_score', color = 'purple')\n",
    "    #ax2.fill_between(\n",
    "    #    fit_times.mean(axis=1),\n",
    "    #    test_scores.mean(axis=1) - test_scores.std(axis=1),\n",
    "    #    test_scores.mean(axis=1) + test_scores.std(axis=1),\n",
    "    #    alpha=0.3,\n",
    "    #    color = 'purple'\n",
    "    #)\n",
    "    ax2.set_ylabel(\"Accuracy\")\n",
    "    ax2.set_xlabel(\"Fit time (s)\")\n",
    "    ax2.set_title(f\"cross_validation_score of {model.__class__.__name__} \")\n",
    "    # show the resulting plots:\n",
    "    plt.show()\n",
    "    save_str = '{}'.format{model}\n",
    "    plt.savefig(\"./\" + save_str + \".png\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Performing parameters tuning for the Random Forest (and maybe the others LT). \n",
    "The choice of the Random Forest (as first model) is because it is the best model, according to the previous analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First create the base model to tune:\n",
    "RF_regressor = RandomForestRegressor(random_state = 10)\n",
    "# Look at parameters used by the learning technique:\n",
    "#pprint(RF_regressor.get_params())\n",
    "\n",
    "# Setting the parameters values to create the grid:\n",
    "# Number of trees in random forest\n",
    "n_estimators = [int(x) for x in np.linspace(start = 200, stop = 2000, num = 10)]\n",
    "# Maximum number of levels in tree\n",
    "max_depth = [int(x) for x in np.linspace(50, 800, num = 8)]\n",
    "# Minimum number of samples required to split a node\n",
    "min_samples_split = [2, 10, 100, 1000]\n",
    "# Minimum number of samples required at each leaf node\n",
    "min_samples_leaf = [1, 4, 10, 50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Random-Grid search CV\n",
    "# Create the random grid\n",
    "#random_grid = {'n_estimators': n_estimators,\n",
    "#               'max_depth': max_depth,\n",
    "#               'min_samples_split': min_samples_split,\n",
    "#               'min_samples_leaf': min_samples_leaf,\n",
    "#               }\n",
    "#\n",
    "## Random search of parameters, using kFold cross validation. Use all available cores in parallel.\n",
    "#RF_random_search = RandomizedSearchCV(  estimator = RF_regressor, \n",
    "#                                        param_distributions = random_grid,\n",
    "#                                        n_iter = 100,   # search across different combinations\n",
    "#                                        scoring = 'neg_mean_absolute_percentage_error', \n",
    "#                                        cv = 4, \n",
    "#                                        verbose = 1, \n",
    "#                                        random_state = 10, \n",
    "#                                        n_jobs = -1,\n",
    "#                                        return_train_score = True,\n",
    "#                                        refit = True\n",
    "#                                        )\n",
    "## Fit the random search model\n",
    "#RF_random_search.fit(features, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can use also the GridSearchCV function, instead of the RandomizedSearchCV.\n",
    "# The code is quite similar as the case before. The paramter grid remains the same.\n",
    "\n",
    "# Create the search grid\n",
    "search_grid = {'n_estimators': n_estimators,\n",
    "               'max_depth': max_depth,\n",
    "               'min_samples_split': min_samples_split,\n",
    "               'min_samples_leaf': min_samples_leaf,\n",
    "               }\n",
    "\n",
    "RF_random_search = GridSearchCV(estimator = RF_regressor, \n",
    "                                param_distributions = search_grid,\n",
    "                                scoring = 'neg_mean_absolute_percentage_error', \n",
    "                                cv = 5, \n",
    "                                n_jobs = -1,\n",
    "                                return_train_score = True,\n",
    "                                refit = True\n",
    "                                )\n",
    "# Fit the grid search model\n",
    "RF_random_search.fit(features, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the best parameters found:\n",
    "print(RF_random_search.best_params_)\n",
    "# get the cross validation score with the best parameters found\n",
    "print(RF_random_search.best_score_)\n",
    "# We can use the function score if the refit parameter is set to True.\n",
    "RF_random_search.score(features, target)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Validation curves for some flexibility parameters. For example n_trees for the random forest (trees in the bag); or the c parameter for SVM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#flexibility_range = np.logspace(200, 1000, 5)\n",
    "flexibility_range = [int(x) for x in np.linspace(start = 200, stop = 1200, num = 6)]\n",
    "print(flexibility_range)\n",
    "\n",
    "train_scores, test_scores = validation_curve(\n",
    "    RandomForestRegressor(),\n",
    "    features,\n",
    "    target,\n",
    "    param_name=\"n_estimators\",\n",
    "    param_range=flexibility_range,\n",
    "    scoring=\"neg_mean_absolute_percentage_error\",\n",
    "    n_jobs=-1,\n",
    ")\n",
    "train_scores_mean = np.mean(train_scores, axis=1)\n",
    "train_scores_std = np.std(train_scores, axis=1)\n",
    "test_scores_mean = np.mean(test_scores, axis=1)\n",
    "test_scores_std = np.std(test_scores, axis=1)\n",
    "\n",
    "plt.title(\"Validation Curve with Random Forest\")\n",
    "plt.xlabel(\"$n_trees$\")\n",
    "plt.ylabel(\"Score\")\n",
    "#plt.ylim(0.0, 1.1)\n",
    "lw = 2\n",
    "plt.semilogx(\n",
    "    flexibility_range, train_scores_mean, label=\"Training score\", color=\"darkorange\", lw=lw\n",
    ")\n",
    "plt.fill_between(\n",
    "    flexibility_range,\n",
    "    train_scores_mean - train_scores_std,\n",
    "    train_scores_mean + train_scores_std,\n",
    "    alpha=0.2,\n",
    "    color=\"darkorange\",\n",
    "    lw=lw,\n",
    ")\n",
    "plt.semilogx(\n",
    "    flexibility_range, test_scores_mean, label=\"Cross-validation score\", color=\"navy\", lw=lw\n",
    ")\n",
    "plt.fill_between(\n",
    "    flexibility_range,\n",
    "    test_scores_mean - test_scores_std,\n",
    "    test_scores_mean + test_scores_std,\n",
    "    alpha=0.2,\n",
    "    color=\"navy\",\n",
    "    lw=lw,\n",
    ")\n",
    "plt.legend(loc=\"best\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "446bff28cea4a303fa29c7e32efeecb1b4166f5c6178c2995e822330431e6a28"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
